Loading training data...
Using 19 files out of 198 (10.0% of dataset)
Found 19 matching SSM-LB file pairs
Loading all data into memory for balanced sampling...
Found 19 files to process
Added 3475 points to the memory
Loaded 1001: 3475 annotated points; (excluded 0 non-annotated)
Loaded 1001: 3475 points
Added 18827 points to the memory
Loaded 2435: 18827 annotated points; (excluded 0 non-annotated)
Loaded 2435: 18827 points
Added 8117 points to the memory
Loaded 1611: 8117 annotated points; (excluded 0 non-annotated)
Loaded 1611: 8117 points
Added 14604 points to the memory
Loaded 1840: 14604 annotated points; (excluded 0 non-annotated)
Loaded 1840: 14604 points
Added 9144 points to the memory
Loaded 1751: 9144 annotated points; (excluded 0 non-annotated)
Loaded 1751: 9144 points
Added 12789 points to the memory
Loaded 0746: 12789 annotated points; (excluded 0 non-annotated)
Loaded 0746: 12789 points
Added 15410 points to the memory
Loaded 0513: 15410 annotated points; (excluded 0 non-annotated)
Loaded 0513: 15410 points
Added 22256 points to the memory
Loaded 1173: 22256 annotated points; (excluded 0 non-annotated)
Loaded 1173: 22256 points
Added 10978 points to the memory
Loaded 1761: 10978 annotated points; (excluded 0 non-annotated)
Loaded 1761: 10978 points
Added 10196 points to the memory
Loaded 1226: 10196 annotated points; (excluded 0 non-annotated)
Loaded 1226: 10196 points
Added 6734 points to the memory
Loaded 0189: 6734 annotated points; (excluded 0 non-annotated)
Loaded 0189: 6734 points
Added 12747 points to the memory
Loaded 0654: 12747 annotated points; (excluded 0 non-annotated)
Loaded 0654: 12747 points
Added 2750 points to the memory
Loaded 1608: 2750 annotated points; (excluded 0 non-annotated)
Loaded 1608: 2750 points
Added 11288 points to the memory
Loaded 0233: 11288 annotated points; (excluded 0 non-annotated)
Loaded 0233: 11288 points
Added 12802 points to the memory
Loaded 1627: 12802 annotated points; (excluded 0 non-annotated)
Loaded 1627: 12802 points
Added 11286 points to the memory
Loaded 2690: 11286 annotated points; (excluded 0 non-annotated)
Loaded 2690: 11286 points
Added 9075 points to the memory
Loaded 1739: 9075 annotated points; (excluded 0 non-annotated)
Loaded 1739: 9075 points
Added 7018 points to the memory
Loaded 2714: 7018 annotated points; (excluded 0 non-annotated)
Loaded 2714: 7018 points
Added 7735 points to the memory
Loaded 0871: 7735 annotated points; (excluded 0 non-annotated)
Loaded 0871: 7735 points

=== Data Loading Summary ===
Total points in dataset: 207231
Non-annotated points (excluded): 0
Annotated points (kept for training): 207231
Filtering ratio: 100.00%

=== Class Distribution ===
Class 0 (non-edge): 194659 points (93.93%)
Class 1 (edge): 12572 points (6.07%)
== Preparing balanced points... ==
Class 0: 194659 points
Class 1: 12572 points
Maximum class size: 194659
Class 0: replicated to 194659 points
Class 1: replicated to 194659 points
Total balanced pool size: 389318
== Finish Preparing balanced points... ==
Found 50 matching SSM-LB file pairs
Loading all data into memory for balanced sampling...
Found 50 files to process
Added 18826 points to the memory
Loaded 2139: 18826 annotated points; (excluded 0 non-annotated)
Loaded 2139: 18826 points
Added 16623 points to the memory
Loaded 1155: 16623 annotated points; (excluded 0 non-annotated)
Loaded 1155: 16623 points
Added 16137 points to the memory
Loaded 0994: 16137 annotated points; (excluded 0 non-annotated)
Loaded 0994: 16137 points
Added 10499 points to the memory
Loaded 1884: 10499 annotated points; (excluded 0 non-annotated)
Loaded 1884: 10499 points
Added 19659 points to the memory
Loaded 1051: 19659 annotated points; (excluded 0 non-annotated)
Loaded 1051: 19659 points
Added 22574 points to the memory
Loaded 2465: 22574 annotated points; (excluded 0 non-annotated)
Loaded 2465: 22574 points
Added 1555 points to the memory
Loaded 2117: 1555 annotated points; (excluded 0 non-annotated)
Loaded 2117: 1555 points
Added 16478 points to the memory
Loaded 1957: 16478 annotated points; (excluded 0 non-annotated)
Loaded 1957: 16478 points
Added 20205 points to the memory
Loaded 2431: 20205 annotated points; (excluded 0 non-annotated)
Loaded 2431: 20205 points
Added 22028 points to the memory
Loaded 2248: 22028 annotated points; (excluded 0 non-annotated)
Loaded 2248: 22028 points
Added 11879 points to the memory
Loaded 0576: 11879 annotated points; (excluded 0 non-annotated)
Loaded 0576: 11879 points
Added 15903 points to the memory
Loaded 0277: 15903 annotated points; (excluded 0 non-annotated)
Loaded 0277: 15903 points
Added 16339 points to the memory
Loaded 2038: 16339 annotated points; (excluded 0 non-annotated)
Loaded 2038: 16339 points
Added 8263 points to the memory
Loaded 1402: 8263 annotated points; (excluded 0 non-annotated)
Loaded 1402: 8263 points
Added 8242 points to the memory
Loaded 1534: 8242 annotated points; (excluded 0 non-annotated)
Loaded 1534: 8242 points
Added 10371 points to the memory
Loaded 0844: 10371 annotated points; (excluded 0 non-annotated)
Loaded 0844: 10371 points
Added 7601 points to the memory
Loaded 1384: 7601 annotated points; (excluded 0 non-annotated)
Loaded 1384: 7601 points
Added 1798 points to the memory
Loaded 1612: 1798 annotated points; (excluded 0 non-annotated)
Loaded 1612: 1798 points
Added 5238 points to the memory
Loaded 1672: 5238 annotated points; (excluded 0 non-annotated)
Loaded 1672: 5238 points
Added 2327 points to the memory
Loaded 1258: 2327 annotated points; (excluded 0 non-annotated)
Loaded 1258: 2327 points
Added 10681 points to the memory
Loaded 0402: 10681 annotated points; (excluded 0 non-annotated)
Loaded 0402: 10681 points
Added 8263 points to the memory
Loaded 2572: 8263 annotated points; (excluded 0 non-annotated)
Loaded 2572: 8263 points
Added 24817 points to the memory
Loaded 0667: 24817 annotated points; (excluded 0 non-annotated)
Loaded 0667: 24817 points
Added 11949 points to the memory
Loaded 2703: 11949 annotated points; (excluded 0 non-annotated)
Loaded 2703: 11949 points
Added 6738 points to the memory
Loaded 2192: 6738 annotated points; (excluded 0 non-annotated)
Loaded 2192: 6738 points
Added 34340 points to the memory
Loaded 0353: 34340 annotated points; (excluded 0 non-annotated)
Loaded 0353: 34340 points
Added 1950 points to the memory
Loaded 0939: 1950 annotated points; (excluded 0 non-annotated)
Loaded 0939: 1950 points
Added 16446 points to the memory
Loaded 0643: 16446 annotated points; (excluded 0 non-annotated)
Loaded 0643: 16446 points
Added 17117 points to the memory
Loaded 1813: 17117 annotated points; (excluded 0 non-annotated)
Loaded 1813: 17117 points
Added 17431 points to the memory
Loaded 1937: 17431 annotated points; (excluded 0 non-annotated)
Loaded 1937: 17431 points
Added 5520 points to the memory
Loaded 0198: 5520 annotated points; (excluded 0 non-annotated)
Loaded 0198: 5520 points
Added 23925 points to the memory
Loaded 2775: 23925 annotated points; (excluded 0 non-annotated)
Loaded 2775: 23925 points
Added 9086 points to the memory
Loaded 0487: 9086 annotated points; (excluded 0 non-annotated)
Loaded 0487: 9086 points
Added 27164 points to the memory
Loaded 1716: 27164 annotated points; (excluded 0 non-annotated)
Loaded 1716: 27164 points
Added 13974 points to the memory
Loaded 1323: 13974 annotated points; (excluded 0 non-annotated)
Loaded 1323: 13974 points
Added 27323 points to the memory
Loaded 1217: 27323 annotated points; (excluded 0 non-annotated)
Loaded 1217: 27323 points
Added 8124 points to the memory
Loaded 2337: 8124 annotated points; (excluded 0 non-annotated)
Loaded 2337: 8124 points
Added 1004 points to the memory
Loaded 0713: 1004 annotated points; (excluded 0 non-annotated)
Loaded 0713: 1004 points
Added 18663 points to the memory
Loaded 2667: 18663 annotated points; (excluded 0 non-annotated)
Loaded 2667: 18663 points
Added 4867 points to the memory
Loaded 2738: 4867 annotated points; (excluded 0 non-annotated)
Loaded 2738: 4867 points
Added 12716 points to the memory
Loaded 0091: 12716 annotated points; (excluded 0 non-annotated)
Loaded 0091: 12716 points
Added 9583 points to the memory
Loaded 2392: 9583 annotated points; (excluded 0 non-annotated)
Loaded 2392: 9583 points
Added 25136 points to the memory
Loaded 2507: 25136 annotated points; (excluded 0 non-annotated)
Loaded 2507: 25136 points
Added 1950 points to the memory
Loaded 1594: 1950 annotated points; (excluded 0 non-annotated)
Loaded 1594: 1950 points
Added 29317 points to the memory
Loaded 1743: 29317 annotated points; (excluded 0 non-annotated)
Loaded 1743: 29317 points
Added 12841 points to the memory
Loaded 0541: 12841 annotated points; (excluded 0 non-annotated)
Loaded 0541: 12841 points
Added 1454 points to the memory
Loaded 1106: 1454 annotated points; (excluded 0 non-annotated)
Loaded 1106: 1454 points
Added 26287 points to the memory
Loaded 0008: 26287 annotated points; (excluded 0 non-annotated)
Loaded 0008: 26287 points
Added 22419 points to the memory
Loaded 1433: 22419 annotated points; (excluded 0 non-annotated)
Loaded 1433: 22419 points
Added 6581 points to the memory
Loaded 0138: 6581 annotated points; (excluded 0 non-annotated)
Loaded 0138: 6581 points

=== Data Loading Summary ===
Total points in dataset: 690211
Non-annotated points (excluded): 0
Annotated points (kept for training): 690211
Filtering ratio: 100.00%

=== Class Distribution ===
Class 0 (non-edge): 650122 points (94.19%)
Class 1 (edge): 40089 points (5.81%)
Training data shape: (391168, 320)
Validation data shape: (692224, 320)
Training labels distribution: [195584 195584]
Grouping features by descriptors...
After grouping - Training data shape: (391168, 20)
After grouping - Validation data shape: (692224, 20)
Starting RL training...
============================================================

--- Episode 1/5 ---
Episode 1 - Epsilon: 1.000 (100.0% random, 0.0% DQN)
Starting with empty feature set...
Initial state shape: (49,)
Step 0: Currently selected descriptors: None
  Action chosen: ADD descriptor 19
  New selection: [19]
  Calculating reward for descriptors: [19]
  Total reward: 1.5044, Per descriptor: 1.5044
Step 1: Currently selected descriptors: [19]
  Action chosen: REMOVE descriptor 19
  New selection: []
  Calculating reward for descriptors: []
  Total reward: 0.0000, Per descriptor: 0.0000
Step 2: Currently selected descriptors: None
  Action chosen: ADD descriptor 3
  New selection: [3]
  Calculating reward for descriptors: [3]
  Total reward: 1.5744, Per descriptor: 1.5744
Step 3: Currently selected descriptors: [3]
  Action chosen: ADD descriptor 15
  New selection: [3, 15]
  Calculating reward for descriptors: [3, 15]
  Total reward: -4.1665, Per descriptor: -2.0832
Step 4: Currently selected descriptors: [3, 15]
  Action chosen: ADD descriptor 17
  New selection: [3, 15, 17]
  Calculating reward for descriptors: [3, 15, 17]
  Total reward: -4.3233, Per descriptor: -1.4411
Step 5: Currently selected descriptors: [3, 15, 17]
  Action chosen: ADD descriptor 6
  New selection: [3, 6, 15, 17]
  Calculating reward for descriptors: [3, 6, 15, 17]
  Total reward: -4.3757, Per descriptor: -1.0939
Step 6: Currently selected descriptors: [3, 6, 15, 17]
  Action chosen: ADD descriptor 4
  New selection: [3, 4, 6, 15, 17]
  Calculating reward for descriptors: [3, 4, 6, 15, 17]
  Total reward: -4.3910, Per descriptor: -0.8782
Step 7: Currently selected descriptors: [3, 4, 6, 15, 17]
  Action chosen: ADD descriptor 19
  New selection: [3, 4, 6, 15, 17, 19]
  Calculating reward for descriptors: [3, 4, 6, 15, 17, 19]
  Total reward: -4.4021, Per descriptor: -0.7337
Step 8: Currently selected descriptors: [3, 4, 6, 15, 17, 19]
  Action chosen: ADD descriptor 10
  New selection: [3, 4, 6, 10, 15, 17, 19]
  Calculating reward for descriptors: [3, 4, 6, 10, 15, 17, 19]
  Total reward: -4.4246, Per descriptor: -0.6321
Step 9: Currently selected descriptors: [3, 4, 6, 10, 15, 17, 19]
  Action chosen: REMOVE descriptor 3
  New selection: [4, 6, 10, 15, 17, 19]
  Calculating reward for descriptors: [4, 6, 10, 15, 17, 19]
  Total reward: -4.4214, Per descriptor: -0.7369
  Updated target network weights
  NEW BEST! Reward: -27.4259 with 6 descriptors
Episode 1 Summary:
  Total reward: -27.4259
  Steps taken: 10
  Final descriptors: 6
  Epsilon: 0.995 (99.5% random, 0.5% DQN)
  Avg last 10 episodes: -27.4259
  Best so far: -27.4259 (6 descriptors)

--- Episode 2/5 ---
Episode 2 - Epsilon: 0.995 (99.5% random, 0.5% DQN)
Starting with empty feature set...
Initial state shape: (49,)
Step 0: Currently selected descriptors: None
  Action chosen: ADD descriptor 16
  New selection: [16]
  Calculating reward for descriptors: [16]
  Total reward: 1.5958, Per descriptor: 1.5958
Step 1: Currently selected descriptors: [16]
  Action chosen: REMOVE descriptor 16
  New selection: []
  Calculating reward for descriptors: []
  Total reward: 0.0000, Per descriptor: 0.0000
Step 2: Currently selected descriptors: None
  Action chosen: ADD descriptor 0
  New selection: [0]
  Calculating reward for descriptors: [0]
  Total reward: 1.6610, Per descriptor: 1.6610
Step 3: Currently selected descriptors: [0]
  Action chosen: ADD descriptor 17
  New selection: [0, 17]
  Calculating reward for descriptors: [0, 17]
  Total reward: -4.7524, Per descriptor: -2.3762
Step 4: Currently selected descriptors: [0, 17]
  Action chosen: ADD descriptor 8
  New selection: [0, 8, 17]
  Calculating reward for descriptors: [0, 8, 17]
  Total reward: -4.5669, Per descriptor: -1.5223
Step 5: Currently selected descriptors: [0, 8, 17]
  Action chosen: ADD descriptor 6
  New selection: [0, 6, 8, 17]
  Calculating reward for descriptors: [0, 6, 8, 17]
  Total reward: -4.5549, Per descriptor: -1.1387
Step 6: Currently selected descriptors: [0, 6, 8, 17]
  Action chosen: ADD descriptor 3
  New selection: [0, 3, 6, 8, 17]
  Calculating reward for descriptors: [0, 3, 6, 8, 17]
  Total reward: -4.5402, Per descriptor: -0.9080
Step 7: Currently selected descriptors: [0, 3, 6, 8, 17]
  Action chosen: ADD descriptor 13
  New selection: [0, 3, 6, 8, 13, 17]
  Calculating reward for descriptors: [0, 3, 6, 8, 13, 17]
  Total reward: -4.5532, Per descriptor: -0.7589
Step 8: Currently selected descriptors: [0, 3, 6, 8, 13, 17]
  Action chosen: REMOVE descriptor 13
  New selection: [0, 3, 6, 8, 17]
  Calculating reward for descriptors: [0, 3, 6, 8, 17]
  Total reward: -4.5402, Per descriptor: -0.9080
Step 9: Currently selected descriptors: [0, 3, 6, 8, 17]
  Action chosen: ADD descriptor 7
  New selection: [0, 3, 6, 7, 8, 17]
  Calculating reward for descriptors: [0, 3, 6, 7, 8, 17]
  Total reward: -4.4137, Per descriptor: -0.7356
Episode 2 Summary:
  Total reward: -28.6647
  Steps taken: 10
  Final descriptors: 6
  Epsilon: 0.990 (99.0% random, 1.0% DQN)
  Avg last 10 episodes: -28.0453
  Best so far: -27.4259 (6 descriptors)

--- Episode 3/5 ---
Episode 3 - Epsilon: 0.990 (99.0% random, 1.0% DQN)
Starting with empty feature set...
Initial state shape: (49,)
Step 0: Currently selected descriptors: None
  Action chosen: ADD descriptor 6
  New selection: [6]
  Calculating reward for descriptors: [6]
  Total reward: 1.5140, Per descriptor: 1.5140
Step 1: Currently selected descriptors: [6]
  Action chosen: ADD descriptor 19
  New selection: [6, 19]
  Calculating reward for descriptors: [6, 19]
  Total reward: -4.5096, Per descriptor: -2.2548
Step 2: Currently selected descriptors: [6, 19]
  Action chosen: ADD descriptor 4
  New selection: [4, 6, 19]
  Calculating reward for descriptors: [4, 6, 19]
  Total reward: -4.5447, Per descriptor: -1.5149
Step 3: Currently selected descriptors: [4, 6, 19]
  Action chosen: ADD descriptor 12
  New selection: [4, 6, 12, 19]
  Calculating reward for descriptors: [4, 6, 12, 19]
  Total reward: -4.3270, Per descriptor: -1.0818
Step 4: Currently selected descriptors: [4, 6, 12, 19]
  Action chosen: REMOVE descriptor 12
  New selection: [4, 6, 19]
  Calculating reward for descriptors: [4, 6, 19]
  Total reward: -4.5447, Per descriptor: -1.5149
Step 5: Currently selected descriptors: [4, 6, 19]
  Action chosen: ADD descriptor 3
  New selection: [3, 4, 6, 19]
  Calculating reward for descriptors: [3, 4, 6, 19]
  Total reward: -4.5245, Per descriptor: -1.1311
Step 6: Currently selected descriptors: [3, 4, 6, 19]
  Action chosen: REMOVE descriptor 3
  New selection: [4, 6, 19]
  Calculating reward for descriptors: [4, 6, 19]
  Total reward: -4.5441, Per descriptor: -1.5147
Step 7: Currently selected descriptors: [4, 6, 19]
  Action chosen: ADD descriptor 3
  New selection: [3, 4, 6, 19]
  Calculating reward for descriptors: [3, 4, 6, 19]
  Total reward: -4.5245, Per descriptor: -1.1311
Step 8: Currently selected descriptors: [3, 4, 6, 19]
  Action chosen: ADD descriptor 1
  New selection: [1, 3, 4, 6, 19]
  Calculating reward for descriptors: [1, 3, 4, 6, 19]
  Total reward: -4.5959, Per descriptor: -0.9192
Step 9: Currently selected descriptors: [1, 3, 4, 6, 19]
  Action chosen: STOP
  Agent chose to STOP
Episode 3 Summary:
  Total reward: -34.6009
  Steps taken: 9
  Final descriptors: 5
  Epsilon: 0.985 (98.5% random, 1.5% DQN)
  Avg last 10 episodes: -30.2305
  Best so far: -27.4259 (6 descriptors)

--- Episode 4/5 ---
Episode 4 - Epsilon: 0.985 (98.5% random, 1.5% DQN)
Starting with empty feature set...
Initial state shape: (49,)
Step 0: Currently selected descriptors: None
  Action chosen: ADD descriptor 4
  New selection: [4]
  Calculating reward for descriptors: [4]
  Total reward: 1.4212, Per descriptor: 1.4212
Step 1: Currently selected descriptors: [4]
  Action chosen: ADD descriptor 5
  New selection: [4, 5]
  Calculating reward for descriptors: [4, 5]
  Total reward: -4.6767, Per descriptor: -2.3384
Step 2: Currently selected descriptors: [4, 5]
  Action chosen: ADD descriptor 14
  New selection: [4, 5, 14]
  Calculating reward for descriptors: [4, 5, 14]
  Total reward: -4.3569, Per descriptor: -1.4523
Step 3: Currently selected descriptors: [4, 5, 14]
  Action chosen: ADD descriptor 8
  New selection: [4, 5, 8, 14]
  Calculating reward for descriptors: [4, 5, 8, 14]
  Total reward: -4.3093, Per descriptor: -1.0773
Step 4: Currently selected descriptors: [4, 5, 8, 14]
  Action chosen: ADD descriptor 18
  New selection: [4, 5, 8, 14, 18]
  Calculating reward for descriptors: [4, 5, 8, 14, 18]
  Total reward: -4.3962, Per descriptor: -0.8792
Step 5: Currently selected descriptors: [4, 5, 8, 14, 18]
  Action chosen: ADD descriptor 19
  New selection: [4, 5, 8, 14, 18, 19]
  Calculating reward for descriptors: [4, 5, 8, 14, 18, 19]
  Total reward: -4.3705, Per descriptor: -0.7284
Step 6: Currently selected descriptors: [4, 5, 8, 14, 18, 19]
  Action chosen: REMOVE descriptor 4
  New selection: [5, 8, 14, 18, 19]
  Calculating reward for descriptors: [5, 8, 14, 18, 19]
  Total reward: -4.3288, Per descriptor: -0.8658
Step 7: Currently selected descriptors: [5, 8, 14, 18, 19]
  Action chosen: ADD descriptor 3
  New selection: [3, 5, 8, 14, 18, 19]
  Calculating reward for descriptors: [3, 5, 8, 14, 18, 19]
  Total reward: -4.3527, Per descriptor: -0.7254
Step 8: Currently selected descriptors: [3, 5, 8, 14, 18, 19]
  Action chosen: ADD descriptor 1
  New selection: [1, 3, 5, 8, 14, 18, 19]
  Calculating reward for descriptors: [1, 3, 5, 8, 14, 18, 19]
  Total reward: -4.4051, Per descriptor: -0.6293
Step 9: Currently selected descriptors: [1, 3, 5, 8, 14, 18, 19]
  Action chosen: ADD descriptor 15
  New selection: [1, 3, 5, 8, 14, 15, 18, 19]
  Calculating reward for descriptors: [1, 3, 5, 8, 14, 15, 18, 19]
  Total reward: -4.3310, Per descriptor: -0.5414
  Training DQN with 39 experiences...
  DQN training loss: 10022766.000000
Episode 4 Summary:
  Total reward: -38.1061
  Steps taken: 10
  Final descriptors: 8
  Epsilon: 0.980 (98.0% random, 2.0% DQN)
  Avg last 10 episodes: -32.1994
  Best so far: -27.4259 (6 descriptors)

--- Episode 5/5 ---
Episode 5 - Epsilon: 0.980 (98.0% random, 2.0% DQN)
Starting with empty feature set...
Initial state shape: (49,)
Step 0: Currently selected descriptors: None
  Action chosen: ADD descriptor 1
  New selection: [1]
  Calculating reward for descriptors: [1]
  Total reward: 1.4773, Per descriptor: 1.4773
Step 1: Currently selected descriptors: [1]
  Action chosen: ADD descriptor 13
  New selection: [1, 13]
  Calculating reward for descriptors: [1, 13]
  Total reward: -4.6078, Per descriptor: -2.3039
Step 2: Currently selected descriptors: [1, 13]
  Action chosen: ADD descriptor 11
  New selection: [1, 11, 13]
  Calculating reward for descriptors: [1, 11, 13]
  Total reward: -4.3772, Per descriptor: -1.4591
Step 3: Currently selected descriptors: [1, 11, 13]
  Action chosen: ADD descriptor 9
  New selection: [1, 9, 11, 13]
  Calculating reward for descriptors: [1, 9, 11, 13]
  Total reward: -4.5665, Per descriptor: -1.1416
Step 4: Currently selected descriptors: [1, 9, 11, 13]
  Action chosen: ADD descriptor 3
  New selection: [1, 3, 9, 11, 13]
  Calculating reward for descriptors: [1, 3, 9, 11, 13]
  Total reward: -4.4762, Per descriptor: -0.8952
Step 5: Currently selected descriptors: [1, 3, 9, 11, 13]
  Action chosen: REMOVE descriptor 9
  New selection: [1, 3, 11, 13]
  Calculating reward for descriptors: [1, 3, 11, 13]
  Total reward: -4.3663, Per descriptor: -1.0916
Step 6: Currently selected descriptors: [1, 3, 11, 13]
  Action chosen: ADD descriptor 6
  New selection: [1, 3, 6, 11, 13]
  Calculating reward for descriptors: [1, 3, 6, 11, 13]
  Total reward: -4.4255, Per descriptor: -0.8851
Step 7: Currently selected descriptors: [1, 3, 6, 11, 13]
  Action chosen: REMOVE descriptor 6
  New selection: [1, 3, 11, 13]
  Calculating reward for descriptors: [1, 3, 11, 13]
  Total reward: -4.3663, Per descriptor: -1.0916
Step 8: Currently selected descriptors: [1, 3, 11, 13]
  Action chosen: REMOVE descriptor 13
  New selection: [1, 3, 11]
  Calculating reward for descriptors: [1, 3, 11]
  Total reward: -4.3221, Per descriptor: -1.4407
Step 9: Currently selected descriptors: [1, 3, 11]
  Action chosen: ADD descriptor 14
  New selection: [1, 3, 11, 14]
  Calculating reward for descriptors: [1, 3, 11, 14]
  Total reward: -4.2455, Per descriptor: -1.0614
  Training DQN with 49 experiences...
  DQN training loss: 7859430.000000
Episode 5 Summary:
  Total reward: -38.2761
  Steps taken: 10
  Final descriptors: 4
  Epsilon: 0.975 (97.5% random, 2.5% DQN)
  Avg last 10 episodes: -33.4147
  Best so far: -27.4259 (6 descriptors)

Training completed!
Best descriptors found: [4, 6, 10, 15, 17, 19]
Number of best descriptors: 6
Final validation accuracy: 0.9751
Selected descriptor indices: [17, 19, 4, 6, 10, 15]
